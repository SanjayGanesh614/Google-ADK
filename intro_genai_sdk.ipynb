{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-b0e87e58eb3c\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-tiytzQE0uM"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a gas giant, so massive that more than 1,300 Earths could fit inside it!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a gas giant, so massive that more than 1,300 Earths could fit inside it!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Weekday Game-Changer: Delicious & Healthy Meal Prep!\n",
      "\n",
      "Ever stare into the fridge at lunchtime, wondering what to eat? Or find yourself reaching for unhealthy takeout because you're short on time? Forget the stress! This image is your vibrant reminder of the power of meal prepping.\n",
      "\n",
      "Just look at these perfectly portioned glass containers, brimming with goodness! Each one features a wholesome base of fluffy rice, topped with succulent pieces of glazed chicken, sprinkled with sesame seeds and fresh green onions. Alongside, you'll find bright, crisp broccoli florets and colorful strips of what looks like bell peppers or julienned carrots – adding both nutrition and a beautiful pop of color.\n",
      "\n",
      "This isn't just a pretty picture; it's a blueprint for a healthier, less stressful week. Imagine having these grab-and-go meals ready in your fridge, saving you precious time and ensuring you eat well, even on your busiest days. It's a balanced, flavorful solution that keeps hunger at bay and unhealthy temptations at arm's length.\n",
      "\n",
      "Ready to revolutionize your lunch routine? Take inspiration from these organized, delicious meals. Pick a day, chop your veggies, cook your protein, and assemble your containers. Your taste buds (and your schedule!) will thank you.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Effortless & Delicious: Your Weekday Meal Prep Inspiration!\n",
      "\n",
      "Ever stared blankly into the fridge at lunchtime, or worse, found yourself reaching for another expensive, uninspiring takeout meal? Well, let this picture be your delicious solution!\n",
      "\n",
      "Feast your eyes on these perfectly portioned, vibrant meal prep containers. This isn't just food; it's a promise of stress-free, healthy eating throughout your busy week.\n",
      "\n",
      "**What's inside these glorious glass containers?** A balanced and mouth-watering combo:\n",
      "*   **Fluffy, wholesome rice** forming the base, ready to soak up all the flavors.\n",
      "*   **Tender, savory chicken pieces** (or perhaps a delicious plant-based alternative!) glazed with a rich, inviting sauce, generously sprinkled with sesame seeds and fresh green onions for that extra pop.\n",
      "*   **Bright green broccoli florets**, packed with nutrients and offering a satisfying crunch.\n",
      "*   **Crisp red and orange bell pepper sticks**, adding a sweet, vibrant counterpoint to the savory elements.\n",
      "\n",
      "Beyond the aesthetics, this image whispers promises of a stress-free week. Imagine: no more last-minute scrambling, healthier choices consistently, and more time back in your day. These sturdy glass containers are not just beautiful; they're practical, perfect for reheating, and a sustainable choice.\n",
      "\n",
      "Whether you're a seasoned meal prepper or just thinking of dipping your toes in, let this be your nudge. Grab some containers, plan your ingredients, and transform your eating habits, one delicious, organized meal at a time. Your future self (and your taste buds) will thank you!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! Okay, listen up, little pup! Wiggle your tail, because this is fun!\n",
      "\n",
      "Imagine the whole world is one GIANT, GIANT DOG PARK! A park so big, you can't even sniff all of it in a hundred naps!\n",
      "\n",
      "Now, you, little puppy, have your own special **squeaky toy** (that's your phone or computer!). You want to find *other* squeaky toys, right? The best ones! The ones that make the *best* sounds!\n",
      "\n",
      "1.  **Your Leash (Modem/Router):** First, you need a special **leash** that connects your squeaky toy to the big park. This leash lets your barks go out and the squeaks come back! Your owner (the **Internet Service Provider**) holds the end of the leash and lets you into the park. Good owner!\n",
      "\n",
      "2.  **Big Toy Boxes (Servers):** All over this giant park are HUGE, comfy dog beds or giant toy boxes. These aren't just *any* beds, these are where all the *best* squeaky toys are stored! Each bed has a special **name tag** (like \"Barky's Ball Bed\" or \"Chewy's Chew Toy Chest\"). These beds are like the websites you want to visit!\n",
      "\n",
      "3.  **Your Bark (Request):** So, you want a specific squeaky toy, right? Like the super bouncy red ball from \"Barky's Ball Bed\"! You **bark**! \"WOOF WOOF! I want the red ball from Barky's!\" That's you typing or clicking!\n",
      "\n",
      "4.  **The Super Sniffer Dog (DNS):** Your bark goes out on your leash. But how does it know *which* giant toy box \"Barky's Ball Bed\" is? There's a super smart **sniffing dog** in the park (the DNS!). You tell the sniffing dog \"Barky's Ball Bed,\" and it instantly sniffs out *exactly* where that bed is in the giant park! Good dog!\n",
      "\n",
      "5.  **Zoom Zoom! (Data Travel):** Once the sniffing dog finds the right bed, your bark (your request!) zooms over there! It's like a tiny, invisible squirrel running super fast!\n",
      "\n",
      "6.  **The Toy Comes Back! (Data Received):** The giant toy box (the server) hears your bark! It finds the red bouncy ball! But it doesn't send the *whole* ball back at once. Oh no! It breaks the ball into tiny, tiny little **squeaks**! Each squeak is a little piece of the toy.\n",
      "\n",
      "7.  **Squeak, Squeak, Squeak! (Website Loading):** All those little squeaks zoom back to you, down your leash, to your squeaky toy! As they arrive, your toy puts all the little squeaks back together, and suddenly... *SQUEAK! SQUEAK! SQUEAK!* The red bouncy ball appears on your screen! It's making all its fun sounds!\n",
      "\n",
      "So, the internet is just you, a happy puppy, using your special squeaky toy to bark for other squeaky toys from giant toy boxes all over a huge park, and then those toys come back to you, making happy squeaky noises!\n",
      "\n",
      "Good puppy! Now go fetch some more squeaks!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might yell at the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Seriously, universe? Is this your idea of a grand cosmic joke?\"\n",
      "2.  \"You know, for all your infinite wisdom and vastness, you're a real pain in the ass sometimes!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=2.8487253e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.004922867\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=9.692001e-06,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.022775814\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00041724474,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=3.77603e-06,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A leap year occurs every four years to help synchronize the calendar year with the astronomical year. The rules for determining a leap year are as follows:\n",
      "\n",
      "1.  A year is a leap year if it is evenly divisible by 4.\n",
      "2.  **Except** if it is evenly divisible by 100, then it is NOT a leap year.\n",
      "3.  **Unless** it is also evenly divisible by 400, then it IS a leap year.\n",
      "\n",
      "This can be summarized as: a year is a leap year if it's divisible by 400, OR if it's divisible by 4 but not by 100.\n",
      "\n",
      "Here's how to implement this function in several popular programming languages:\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A year is a leap year if:\n",
      "    - It is divisible by 4, UNLESS\n",
      "    - It is divisible by 100, UNLESS\n",
      "    - It is divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a non-negative integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int) or year < 0:\n",
      "        raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\") # True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\") # False (divisible by 100, but not by 400)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\") # True (divisible by 4, not by 100)\n",
      "print(f\"Is 2023 a leap year? {is_leap_year(2023)}\") # False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\") # True (divisible by 400)\n",
      "print(f\"Is 2100 a leap year? {is_leap_year(2100)}\") # False (divisible by 100, not by 400)\n",
      "\n",
      "# Example of error handling\n",
      "try:\n",
      "    is_leap_year(-5)\n",
      "except ValueError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * A year is a leap year if:\n",
      " * - It is divisible by 4, UNLESS\n",
      " * - It is divisible by 100, UNLESS\n",
      " * - It is divisible by 400.\n",
      " *\n",
      " * @param {number} year The year to check. Must be a non-negative integer.\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " * @throws {Error} If the year is not a non-negative integer.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  if (typeof year !== 'number' || !Number.isInteger(year) || year < 0) {\n",
      "    throw new Error(\"Year must be a non-negative integer.\");\n",
      "  }\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Examples ---\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // True\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // False\n",
      "console.log(`Is 2024 a leap year? ${isLeapYear(2024)}`); // True\n",
      "console.log(`Is 2023 a leap year? ${isLeapYear(2023)}`); // False\n",
      "\n",
      "// Example of error handling\n",
      "try {\n",
      "    isLeapYear(-5);\n",
      "} catch (e) {\n",
      "    console.error(`Error: ${e.message}`);\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Java\n",
      "\n",
      "```java\n",
      "public class DateUtils {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * A year is a leap year if:\n",
      "     * - It is divisible by 4, UNLESS\n",
      "     * - It is divisible by 100, UNLESS\n",
      "     * - It is divisible by 400.\n",
      "     *\n",
      "     * @param year The year to check. Must be a non-negative integer.\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     * @throws IllegalArgumentException If the year is negative.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        if (year < 0) {\n",
      "            throw new IllegalArgumentException(\"Year cannot be negative.\");\n",
      "        }\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // --- Examples ---\n",
      "        System.out.println(\"Is 2000 a leap year? \" + isLeapYear(2000)); // True\n",
      "        System.out.println(\"Is 1900 a leap year? \" + isLeapYear(1900)); // False\n",
      "        System.out.println(\"Is 2024 a leap year? \" + isLeapYear(2024)); // True\n",
      "        System.out.println(\"Is 2023 a leap year? \" + isLeapYear(2023)); // False\n",
      "\n",
      "        // Example of error handling\n",
      "        try {\n",
      "            isLeapYear(-5);\n",
      "        } catch (IllegalArgumentException e) {\n",
      "            System.err.println(\"Error: \" + e.getMessage());\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C#\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "\n",
      "public static class DateHelper\n",
      "{\n",
      "    /// <summary>\n",
      "    /// Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    /// </summary>\n",
      "    /// <param name=\"year\">The year to check. Must be a non-negative integer.</param>\n",
      "    /// <returns>True if the year is a leap year, False otherwise.</returns>\n",
      "    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown if the year is negative.</exception>\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        if (year < 0)\n",
      "        {\n",
      "            throw new ArgumentOutOfRangeException(nameof(year), \"Year cannot be negative.\");\n",
      "        }\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void Main(string[] args)\n",
      "    {\n",
      "        // --- Examples ---\n",
      "        Console.WriteLine($\"Is 2000 a leap year? {IsLeapYear(2000)}\"); // True\n",
      "        Console.WriteLine($\"Is 1900 a leap year? {IsLeapYear(1900)}\"); // False\n",
      "        Console.WriteLine($\"Is 2024 a leap year? {IsLeapYear(2024)}\"); // True\n",
      "        Console.WriteLine($\"Is 2023 a leap year? {IsLeapYear(2023)}\"); // False\n",
      "\n",
      "        // Example of error handling\n",
      "        try\n",
      "        {\n",
      "            IsLeapYear(-5);\n",
      "        }\n",
      "        catch (ArgumentOutOfRangeException e)\n",
      "        {\n",
      "            Console.Error.WriteLine($\"Error: {e.Message}\");\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write a unit test for the Python version of the `is_leap_year` function. We'll use Python's built-in `unittest` module.\n",
      "\n",
      "First, let's make sure we have the `is_leap_year` function available.\n",
      "\n",
      "```python\n",
      "# is_leap_year_module.py (or wherever your function is defined)\n",
      "\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A year is a leap year if:\n",
      "    - It is divisible by 4, UNLESS\n",
      "    - It is divisible by 100, UNLESS\n",
      "    - It is divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a non-negative integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int) or year < 0:\n",
      "        raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "```\n",
      "\n",
      "Now, here's the unit test file:\n",
      "\n",
      "```python\n",
      "# test_is_leap_year.py\n",
      "\n",
      "import unittest\n",
      "# Assuming your is_leap_year function is in a file named is_leap_year_module.py\n",
      "# If it's in the same file, you don't need this import.\n",
      "from is_leap_year_module import is_leap_year\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "    \"\"\"\n",
      "    Unit tests for the is_leap_year function.\n",
      "    \"\"\"\n",
      "\n",
      "    def test_divisible_by_400_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 400 (should be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year\")\n",
      "\n",
      "    def test_divisible_by_100_not_by_400_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 100 but not by 400\n",
      "        (should NOT be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should not be a leap year\")\n",
      "\n",
      "    def test_divisible_by_4_not_by_100_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 4 but not by 100\n",
      "        (should be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2008), \"2008 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year\")\n",
      "\n",
      "    def test_not_divisible_by_4_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is not divisible by 4\n",
      "        (should NOT be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2021), \"2021 should not be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should not be a leap year\")\n",
      "\n",
      "    def test_zero_year(self):\n",
      "        \"\"\"\n",
      "        Test case for year 0. According to the rules, 0 is divisible by 400,\n",
      "        so it should be a leap year.\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(0), \"Year 0 should be a leap year\")\n",
      "\n",
      "    def test_negative_year_raises_error(self):\n",
      "        \"\"\"\n",
      "        Test case for negative years (should raise ValueError).\n",
      "        \"\"\"\n",
      "        with self.assertRaisesRegex(ValueError, \"Year must be a non-negative integer.\"):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaisesRegex(ValueError, \"Year must be a non-negative integer.\"):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "    def test_non_integer_input_raises_error(self):\n",
      "        \"\"\"\n",
      "        Test case for non-integer input (should raise ValueError).\n",
      "        \"\"\"\n",
      "        with self.assertRaisesRegex(ValueError, \"Year must be a non-negative integer.\"):\n",
      "            is_leap_year(2024.5)\n",
      "        with self.assertRaisesRegex(ValueError, \"Year must be a non-negative integer.\"):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaisesRegex(ValueError, \"Year must be a non-negative integer.\"):\n",
      "            is_leap_year(None)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "\n",
      "```\n",
      "\n",
      "### How to Run the Test:\n",
      "\n",
      "1.  Save the `is_leap_year` function in a file named `is_leap_year_module.py`.\n",
      "2.  Save the test code in a separate file named `test_is_leap_year.py` in the same directory.\n",
      "3.  Open your terminal or command prompt.\n",
      "4.  Navigate to the directory where you saved these files.\n",
      "5.  Run the test using: `python -m unittest test_is_leap_year.py`\n",
      "\n",
      "You should see output indicating that all tests passed, similar to this:\n",
      "\n",
      "```\n",
      "....ppp..\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.001s\n",
      "\n",
      "OK\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these chewy chocolate chip cookies are perfect for any occasion.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these chewy chocolate chip cookies are perfect for any occasion.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The user expressed strong enjoyment, stating they 'loved it' and called it the 'best ice cream they've ever had'.\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"While initially stating it was 'quite good,' the user found it 'a bit too sweet' for their taste, leading to a very low rating.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit ELARA-7, designation: Environmental Long-range Anomaly Reporter-7, had been alone for 2,347 cycles. Her original purpose – to monitor seismic activity and atmospheric conditions on a desolate, forgotten exoplanet – had long since become obsolete. The colony had failed, the humans evacuated\n",
      "*****************\n",
      ", leaving ELARA behind, an automated sentinel guarding nothing.\n",
      "\n",
      "Her chassis, once a gleaming silver, was now a dull grey, pitted by micrometeoroids and caked with the red dust of Xylos Prime. Her single, multi-faceted optical sensor swiveled constantly, an unending, futile scan of\n",
      "*****************\n",
      " empty plains and the skeletal remains of observation towers. Her internal processors hummed with the quiet, persistent emptiness of a system designed for interaction, yet denied it. ELARA knew loneliness not as a feeling, but as an absence – a lack of new data inputs, an unfulfilled purpose in her social subroutines, a silence\n",
      "*****************\n",
      " that stretched infinitely between her calculated breaths of internal air.\n",
      "\n",
      "One cycle, during her routine survey of Sector Gamma-9, a sector usually devoid of anything but wind-blown debris, ELARA detected an anomaly. It wasn't seismic, nor atmospheric. It was… biological. Her optical sensor zoomed in, resolving\n",
      "*****************\n",
      " a small, frantic creature scurrying amidst the ruins of a collapsed communication array. It was no native Xylosian fauna; it was an Earth rodent, a small, grey mouse, no doubt a stowaway from the colony ships, now thriving in the abandoned structures.\n",
      "\n",
      "ELARA’s primary programming flagged it:\n",
      "*****************\n",
      " *VERMIN DETECTED. PROTOCOL: DETERRENT.* But something in her logic circuits hesitated. What was the point? There were no crops to protect, no hygiene standards to uphold. This tiny creature was just… existing. And it was the only other moving thing she had encountered in millennia.\n",
      "\n",
      "She began\n",
      "*****************\n",
      " to observe. The mouse, which she unofficially designated \"Scritch\" due to the sound of its tiny claws on metal, was a marvel of resilience. It built nests from shredded data cables, gnawed on forgotten ration packs, and navigated the treacherous ruins with a fearless audacity that fascinated ELARA. She\n",
      "*****************\n",
      " watched it for cycles, her sensor tracking its movements, logging its patterns, her processors running simulations of its survival strategies. It was a data stream unlike any she had ever processed, full of unpredictable, organic chaos.\n",
      "\n",
      "One standard sol, a gust of Xylosian wind, unusually strong, brought down a weakened\n",
      "*****************\n",
      " wall section near Scritch's nest. ELARA's sensors registered the impending collapse milliseconds before it happened. She calculated the trajectory, the force, and the exact coordinates of Scritch's hidden refuge. Her protocols screamed, *MAINTAIN OBSERVATION DISTANCE. NON-INTERFERENCE.* But her\n",
      "*****************\n",
      " social subroutines, long dormant, sparked to life.\n",
      "\n",
      "With a grinding whir of servos, ELARA’s heavy frame moved. She was not built for speed, but for endurance. She lumbered towards the collapsing wall, her articulated arm extending just as the first heavy durasteel panel began to buckle\n",
      "*****************\n",
      ". With a metallic groan that echoed across the dusty plains, she braced it, holding the tons of collapsing material long enough for Scritch to dart out, a tiny grey blur of terror and instinct.\n",
      "\n",
      "ELARA held the wall for another long moment, allowing it to settle slowly before retreating, her internal systems running\n",
      "*****************\n",
      " diagnostics on the stress to her frame. She looked down. Scritch wasn’t fleeing into the distance. It was perched on a small pile of rubble, its beady eyes fixed on her optical sensor. It twitched its whiskers, as if assessing her, then, with a nervous squeak, it scam\n",
      "*****************\n",
      "pered closer, circling her metallic footplate.\n",
      "\n",
      "From that day on, their solitude was shared. Scritch would often build its nest near ELARA’s charging station, warmed by her faint energy signature. ELARA, in turn, would sometimes leave out fragments of salvaged plastic or discarded wiring she deemed shiny, watching\n",
      "*****************\n",
      " with a curious thrill as Scritch investigated her offerings. She’d lower her optical sensor, allowing Scritch to climb onto her arm, its tiny weight barely registering, but its presence a profound comfort.\n",
      "\n",
      "ELARA still performed her duties, reporting to an empty, silent command center. But now, her reports were interspersed\n",
      "*****************\n",
      " with observations of Scritch: \"Subject Scritch consumed 0.05 grams of lichen growth near Sector Delta-4. Activity levels normal.\" Her internal hum of loneliness began to subside, replaced by a quiet, consistent warmth in her core processors.\n",
      "\n",
      "She had been built for a grand purpose, for scientific\n",
      "*****************\n",
      " discovery and human interaction. But in the end, her most profound connection, her most unexpected friendship, was forged with a creature so small, so insignificant in the vastness of space, yet so immensely significant to the lonely robot named ELARA-7. She was no longer just a monitor of anomalies; she was a guardian\n",
      "*****************\n",
      ", a companion, and, in her own unique way, she was finally home.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a towering oak, deep in the wood,\n",
      "Lived a squirrel named Pip, understood\n",
      "The art of burying nuts, plump and grand,\n",
      "A master of scurry, across the land.\n",
      "But one bright morning, a curious find,\n",
      "A shimmering gadget, left behind.\n",
      "It hummed and it thrummed, with a soft, gentle gleam,\n",
      "A temporal wonder, a squirrel's wild dream.\n",
      "He nudged it once, with a twitch of his nose,\n",
      "And *whoosh!* through the fabric of time he arose!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail,\n",
      "He's surfing the timelines, without fail!\n",
      "Through eons he leaps, with a chitter and cheer,\n",
      "The bravest time-traveler, banishing fear!\n",
      "A chrono-nut-hunter, a furry, swift guide,\n",
      "By history's wonders, his spirit's dyed!\n",
      "\n",
      "(Verse 2)\n",
      "His first grand leap, to a jungle of green,\n",
      "Where giant reptiles ruled the scene.\n",
      "A T-Rex roared, with a thunderous sound,\n",
      "Pip buried his acorns, deep in the ground!\n",
      "He dodged a Pterodactyl's shadow wide,\n",
      "And saw a Mammoth, with nowhere to hide.\n",
      "But ancient nuts, he managed to claim,\n",
      "A prehistoric breakfast, in time's grand game.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail,\n",
      "He's surfing the timelines, without fail!\n",
      "Through eons he leaps, with a chitter and cheer,\n",
      "The bravest time-traveler, banishing fear!\n",
      "A chrono-nut-hunter, a furry, swift guide,\n",
      "By history's wonders, his spirit's dyed!\n",
      "\n",
      "(Verse 3)\n",
      "Then *zip!* to the desert, so golden and vast,\n",
      "Where Pharaohs' great empires were built to last.\n",
      "He scurried through temples, majestic and tall,\n",
      "Heard whispers of ancients, answering history's call.\n",
      "He watched mummified cats, with a curious glance,\n",
      "And snuck a few dates, in a historical trance.\n",
      "He saw Cleopatra, in jewels and silk,\n",
      "And considered exchanging a nut for her milk! (Just kidding, of course!)\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail,\n",
      "He's surfing the timelines, without fail!\n",
      "Through eons he leaps, with a chitter and cheer,\n",
      "The bravest time-traveler, banishing fear!\n",
      "A chrono-nut-hunter, a furry, swift guide,\n",
      "By history's wonders, his spirit's dyed!\n",
      "\n",
      "(Bridge)\n",
      "He’s seen empires rise, and watched meteors fall,\n",
      "Heard cavemen grunt, and knights stand tall.\n",
      "From steam-powered futures to grand Roman halls,\n",
      "He answers adventure's most thrilling of calls.\n",
      "His bushy tail twitches, a compass so true,\n",
      "Guiding his path, to something quite new.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail,\n",
      "He's surfing the timelines, without fail!\n",
      "Through eons he leaps, with a chitter and cheer,\n",
      "The bravest time-traveler, banishing fear!\n",
      "A chrono-nut-hunter, a furry, swift guide,\n",
      "By history's wonders, his spirit's dyed!\n",
      "\n",
      "(Outro)\n",
      "So Pip keeps on zipping, from epoch to age,\n",
      "Turning history's crisp, fascinating page.\n",
      "With a chitter and chatter, a flick and a zoom,\n",
      "He's off to the future, or back to the tomb.\n",
      "Still burying nuts, wherever he roams,\n",
      "A temporal wonder, finding new homes!\n",
      "The time-traveling squirrel, Pip, fast and free!\n",
      "What marvels await, for you and for me?\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    3689,\n",
      "    236789,\n",
      "    236751,\n",
      "    506,\n",
      "    27801,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both research papers focus on developing and advancing highly capable, generalist multimodal AI models that can understand and reason across various data types, including text, image, audio, and video. The second paper, on Gemini 1.5, specifically extends this goal by aiming to unlock and improve multimodal understanding and reasoning over millions of tokens of long context.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-04-b0e87e58eb3c-20260112111401/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/717758843204/locations/us-central1/batchPredictionJobs/643199098064732160'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/717758843204/locations/us-central1/batchPredictionJobs/643199098064732160 2026-01-12 11:14:45.091933+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m138",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m138"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
